{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6a0eeb-69ab-45ea-9c90-ea8cb19cf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1da49a-bb50-46e8-8ca9-cf79612fa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import kaggle\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch.nn.init as init\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf2f713-d4f1-4084-b06d-863ce60a0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ece220d-57c4-4d07-a9fb-59a277b385bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Apply Kaiming initialization\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769f190f-f6f6-4a30-82b5-75abf58572f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.conv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Apply Kaiming initialization\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), 256, 4, 4)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ae4d59-5c97-485e-8d67-eaff32b1ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5128413f-7a50-4d3a-bd40-13ab997e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def train(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_bce_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss, bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_bce_loss += bce.item()\n",
    "        train_kld_loss += kld.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return train_loss / len(dataloader.dataset), train_bce_loss / len(dataloader.dataset), train_kld_loss / len(dataloader.dataset)\n",
    "\n",
    "def generate_images(model, latent_dim, num_images, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, latent_dim).to(device)\n",
    "        sample = model.decoder(z)\n",
    "    return sample\n",
    "\n",
    "def interpolate_latent(model, start_image, end_image, num_steps, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_mu, start_logvar = model.encoder(start_image.to(device))\n",
    "        end_mu, end_logvar = model.encoder(end_image.to(device))\n",
    "        \n",
    "        start_z = model.reparameterize(start_mu, start_logvar)\n",
    "        end_z = model.reparameterize(end_mu, end_logvar)\n",
    "        \n",
    "        interpolated_z = torch.zeros(num_steps, start_z.size(1)).to(device)\n",
    "        for i in range(num_steps):\n",
    "            alpha = i / (num_steps - 1)\n",
    "            interpolated_z[i] = start_z * (1 - alpha) + end_z * alpha\n",
    "        print(interpolated_z.shape)\n",
    "        interpolated_images = model.decoder(interpolated_z)\n",
    "    return interpolated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4712be3d-6126-4d80-95ae-29719a12d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(dataset_name, download_path):\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "    \n",
    "    subfolder = os.path.join(download_path, 'cats')\n",
    "    for item in os.listdir(subfolder):\n",
    "        s = os.path.join(subfolder, item)\n",
    "        d = os.path.join(download_path, item)\n",
    "        shutil.move(s, d)\n",
    "    os.rmdir(subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a08a2b-a3f7-4f05-bf8d-a64ee242f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def preprocess_cat_dataset(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Extracts .jpg files from all subdirectories in the source directory\n",
    "    and copies them to the target directory.\n",
    "    \n",
    "    :param source_dir: Path to the source directory containing CAT_XX subdirectories\n",
    "    :param target_dir: Path to the target directory where .jpg files will be copied\n",
    "    \"\"\"\n",
    "    # Create the target directory if it doesn't exist\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Iterate through all subdirectories in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                source_file = Path(root) / file\n",
    "                target_file = Path(target_dir) / file\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file, target_file)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Print progress every 100 files\n",
    "                if processed_count % 100 == 0:\n",
    "                    print(f\"Processed {processed_count} images...\")\n",
    "\n",
    "    print(f\"Finished processing. Total images copied: {processed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a87754-09b9-4eba-aad2-0ae47c4477f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_dir, dataset_name='crawford/cat-dataset', batch_size=64, latent_dim=200, lr=1e-3, epochs=50, device='cuda'):\n",
    "    # Download the dataset if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"Downloading dataset '{dataset_name}'...\")\n",
    "        download_kaggle_dataset(dataset_name, data_dir)\n",
    "        print(\"Dataset downloaded successfully.\")\n",
    "    if not os.path.exists(\"./cleaned_data\"):\n",
    "        preprocess_cat_dataset(data_dir, \"./cleaned_data\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = CatDataset(\"./cleaned_data\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    encoder = Encoder(latent_dim)\n",
    "    decoder = Decoder(latent_dim)\n",
    "\n",
    "    model = VAE(encoder, decoder).to(device)\n",
    "    if os.path.exists('./best_model.pth'):\n",
    "        model_path = 'best_model.pth'\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    bce_losses = []\n",
    "    kld_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, bce_loss, kld_loss = train(model, dataloader, optimizer, device, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        bce_losses.append(bce_loss)\n",
    "        kld_losses.append(kld_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}, BCE: {bce_loss:.4f}, KLD: {kld_loss:.4f}')\n",
    "        \n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            torch.save(model.state_dict(), f'model_checkpoint_{epoch}.pth')\n",
    "    return train_losses, bce_losses, kld_losses, dataset, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e073478-50e4-4a21-a9ad-a9b2b5464c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6740.6242, BCE: 6607.0024, KLD: 133.6218\n",
      "Epoch 2, Loss: 6738.9783, BCE: 6605.3345, KLD: 133.6439\n",
      "Epoch 3, Loss: 6736.9873, BCE: 6602.9424, KLD: 134.0449\n",
      "Epoch 4, Loss: 6737.8102, BCE: 6604.1674, KLD: 133.6428\n",
      "Epoch 5, Loss: 6737.7562, BCE: 6603.5341, KLD: 134.2222\n",
      "Epoch 6, Loss: 6735.6423, BCE: 6601.7861, KLD: 133.8562\n",
      "Epoch 7, Loss: 6737.4368, BCE: 6603.3750, KLD: 134.0618\n",
      "Epoch 8, Loss: 6736.9534, BCE: 6602.9816, KLD: 133.9719\n",
      "Epoch 9, Loss: 6736.5268, BCE: 6602.4356, KLD: 134.0911\n",
      "Epoch 10, Loss: 6734.9486, BCE: 6601.0006, KLD: 133.9480\n",
      "Epoch 11, Loss: 6735.7738, BCE: 6601.5069, KLD: 134.2670\n",
      "Epoch 12, Loss: 6734.8999, BCE: 6600.7592, KLD: 134.1408\n",
      "Epoch 13, Loss: 6733.6245, BCE: 6599.4678, KLD: 134.1567\n",
      "Epoch 14, Loss: 6733.3773, BCE: 6599.5233, KLD: 133.8539\n",
      "Epoch 15, Loss: 6733.4519, BCE: 6599.1903, KLD: 134.2616\n",
      "Epoch 16, Loss: 6732.9177, BCE: 6598.7426, KLD: 134.1751\n",
      "Epoch 17, Loss: 6734.1612, BCE: 6599.8505, KLD: 134.3107\n",
      "Epoch 18, Loss: 6735.8329, BCE: 6601.5069, KLD: 134.3260\n",
      "Epoch 19, Loss: 6733.1332, BCE: 6598.7557, KLD: 134.3774\n",
      "Epoch 20, Loss: 6732.3490, BCE: 6597.9516, KLD: 134.3974\n",
      "Epoch 21, Loss: 6731.6033, BCE: 6597.0442, KLD: 134.5591\n",
      "Epoch 22, Loss: 6731.8605, BCE: 6597.5335, KLD: 134.3270\n",
      "Epoch 23, Loss: 6731.1866, BCE: 6596.5927, KLD: 134.5940\n",
      "Epoch 24, Loss: 6730.5453, BCE: 6596.1680, KLD: 134.3773\n",
      "Epoch 25, Loss: 6731.8151, BCE: 6597.3427, KLD: 134.4724\n",
      "Epoch 26, Loss: 6730.7287, BCE: 6596.3939, KLD: 134.3349\n",
      "Epoch 27, Loss: 6732.7007, BCE: 6598.3870, KLD: 134.3137\n",
      "Epoch 28, Loss: 6729.2472, BCE: 6594.6042, KLD: 134.6431\n",
      "Epoch 29, Loss: 6728.9597, BCE: 6594.6009, KLD: 134.3588\n",
      "Epoch 30, Loss: 6729.0614, BCE: 6594.6137, KLD: 134.4478\n",
      "Epoch 31, Loss: 6730.0410, BCE: 6595.6376, KLD: 134.4034\n",
      "Epoch 32, Loss: 6728.8321, BCE: 6594.1296, KLD: 134.7025\n",
      "Epoch 33, Loss: 6728.0614, BCE: 6593.4853, KLD: 134.5760\n",
      "Epoch 34, Loss: 6728.3673, BCE: 6593.7165, KLD: 134.6508\n",
      "Epoch 35, Loss: 6729.5042, BCE: 6594.8804, KLD: 134.6238\n",
      "Epoch 36, Loss: 6726.6800, BCE: 6592.0406, KLD: 134.6394\n",
      "Epoch 37, Loss: 6729.0250, BCE: 6594.2527, KLD: 134.7723\n",
      "Epoch 38, Loss: 6727.6553, BCE: 6592.9615, KLD: 134.6938\n",
      "Epoch 39, Loss: 6726.6588, BCE: 6592.1516, KLD: 134.5072\n",
      "Epoch 40, Loss: 6727.4222, BCE: 6592.6533, KLD: 134.7690\n",
      "Epoch 41, Loss: 6727.0812, BCE: 6592.2039, KLD: 134.8773\n",
      "Epoch 42, Loss: 6725.7269, BCE: 6591.0466, KLD: 134.6803\n",
      "Epoch 43, Loss: 6727.8183, BCE: 6593.1188, KLD: 134.6996\n",
      "Epoch 44, Loss: 6725.8993, BCE: 6591.0199, KLD: 134.8794\n",
      "Epoch 45, Loss: 6724.2858, BCE: 6589.5184, KLD: 134.7674\n",
      "Epoch 46, Loss: 6725.9480, BCE: 6590.9388, KLD: 135.0092\n",
      "Epoch 47, Loss: 6726.0117, BCE: 6591.2390, KLD: 134.7727\n",
      "Epoch 48, Loss: 6724.8671, BCE: 6590.1294, KLD: 134.7377\n",
      "Epoch 49, Loss: 6726.0141, BCE: 6591.2815, KLD: 134.7326\n",
      "Epoch 50, Loss: 6725.6941, BCE: 6591.0207, KLD: 134.6734\n"
     ]
    }
   ],
   "source": [
    "train_losses, bce_losses, kld_losses, dataset, decoder = main('./dataset', device='mps')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Total Loss')\n",
    "plt.plot(bce_losses, label='BCE Loss')\n",
    "plt.plot(kld_losses, label='KLD Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot_2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55ce17a3-d8c2-4d2a-aad8-5d3599928be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Total Loss')\n",
    "plt.plot(bce_losses, label='BCE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot_2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7153735d-caef-4ef9-8d30-85ca45741a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 200 \n",
    "# Initialize the Decoder model\n",
    "encoder = Encoder(latent_dim)\n",
    "decoder = Decoder(latent_dim)\n",
    "device = 'mps'\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "model_path = 'best_model.pth'  \n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "new_images = generate_images(model, latent_dim, 16, device)\n",
    "save_image(new_images, 'generated_images.png', nrow=4)\n",
    "\n",
    "start_image = dataset[0].unsqueeze(0).to(device)\n",
    "end_image = dataset[1].unsqueeze(0).to(device)\n",
    "interpolated_images = interpolate_latent(model, start_image, end_image, 10, device)\n",
    "save_image(interpolated_images, 'interpolated_images.png', nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f253f505-e351-4165-910c-b6ccacd60c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stock_price = np.array([20, 30, 5, 10, 40, 20, 70])\n",
    "\n",
    "start_price = stock_price[0]\n",
    "profit = 0\n",
    "\n",
    "for i in range(1,len(stock_price)):\n",
    "    if stock_price[i] > stock_price[i-1]:\n",
    "        profit += stock_price[i] - stock_price[i-1]\n",
    "    else:\n",
    "        start_price = stock_price[i]\n",
    "\n",
    "print(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f70c5ed4-7d07-4abe-9cd8-9ab950549e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.random((10,5))\n",
    "y = np.array([1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2254d93-3b58-4a17-bfda-209fd2f84e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 , 10, 5, 1\n",
    "W_hidden_01 = np.random.normal((5, 10))\n",
    "W_hidden_12 = np.random.normal((10, 5))\n",
    "W_output  = np.random.normal((5,1))\n",
    "\n",
    "b_hidden_01 = np.zeros((10,1))\n",
    "b_hidden_12 = np.zeros((5,1))\n",
    "\n",
    "activation = np.tanh\n",
    "\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    h1_out = np.matmul(X, W_hidden_01) + b_hidden_01\n",
    "    h1_act = activation(h1_out)\n",
    "\n",
    "    h2_out = np.matmul(h1_act, W_hidden_12) + b_hidden_12\n",
    "    h2_act = activation(h2_out)\n",
    "\n",
    "    out = np.matmul(h2_act, W_output)\n",
    "\n",
    "    out_p = np.exp()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
