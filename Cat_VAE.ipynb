{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6a0eeb-69ab-45ea-9c90-ea8cb19cf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1da49a-bb50-46e8-8ca9-cf79612fa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import kaggle\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch.nn.init as init\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf2f713-d4f1-4084-b06d-863ce60a0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ece220d-57c4-4d07-a9fb-59a277b385bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Apply Kaiming initialization\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769f190f-f6f6-4a30-82b5-75abf58572f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.conv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Apply Kaiming initialization\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), 256, 4, 4)\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ae4d59-5c97-485e-8d67-eaff32b1ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5128413f-7a50-4d3a-bd40-13ab997e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def train(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_bce_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss, bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_bce_loss += bce.item()\n",
    "        train_kld_loss += kld.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return train_loss / len(dataloader.dataset), train_bce_loss / len(dataloader.dataset), train_kld_loss / len(dataloader.dataset)\n",
    "\n",
    "def generate_images(model, latent_dim, num_images, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, latent_dim).to(device)\n",
    "        sample = model.decoder(z)\n",
    "    return sample\n",
    "\n",
    "def interpolate_latent(model, start_image, end_image, num_steps, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_mu, start_logvar = model.encoder(start_image.to(device))\n",
    "        end_mu, end_logvar = model.encoder(end_image.to(device))\n",
    "        \n",
    "        start_z = model.reparameterize(start_mu, start_logvar)\n",
    "        end_z = model.reparameterize(end_mu, end_logvar)\n",
    "        \n",
    "        interpolated_z = torch.zeros(num_steps, start_z.size(1)).to(device)\n",
    "        for i in range(num_steps):\n",
    "            alpha = i / (num_steps - 1)\n",
    "            interpolated_z[i] = start_z * (1 - alpha) + end_z * alpha\n",
    "        print(interpolated_z.shape)\n",
    "        interpolated_images = model.decoder(interpolated_z)\n",
    "    return interpolated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4712be3d-6126-4d80-95ae-29719a12d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(dataset_name, download_path):\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "    \n",
    "    subfolder = os.path.join(download_path, 'cats')\n",
    "    for item in os.listdir(subfolder):\n",
    "        s = os.path.join(subfolder, item)\n",
    "        d = os.path.join(download_path, item)\n",
    "        shutil.move(s, d)\n",
    "    os.rmdir(subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a08a2b-a3f7-4f05-bf8d-a64ee242f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def preprocess_cat_dataset(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Extracts .jpg files from all subdirectories in the source directory\n",
    "    and copies them to the target directory.\n",
    "    \n",
    "    :param source_dir: Path to the source directory containing CAT_XX subdirectories\n",
    "    :param target_dir: Path to the target directory where .jpg files will be copied\n",
    "    \"\"\"\n",
    "    # Create the target directory if it doesn't exist\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Iterate through all subdirectories in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                source_file = Path(root) / file\n",
    "                target_file = Path(target_dir) / file\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file, target_file)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Print progress every 100 files\n",
    "                if processed_count % 100 == 0:\n",
    "                    print(f\"Processed {processed_count} images...\")\n",
    "\n",
    "    print(f\"Finished processing. Total images copied: {processed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a87754-09b9-4eba-aad2-0ae47c4477f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_dir, dataset_name='crawford/cat-dataset', batch_size=64, latent_dim=200, lr=1e-3, epochs=50, device='cuda'):\n",
    "    # Download the dataset if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"Downloading dataset '{dataset_name}'...\")\n",
    "        download_kaggle_dataset(dataset_name, data_dir)\n",
    "        print(\"Dataset downloaded successfully.\")\n",
    "    if not os.path.exists(\"./cleaned_data\"):\n",
    "        preprocess_cat_dataset(data_dir, \"./cleaned_data\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = CatDataset(\"./cleaned_data\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    encoder = Encoder(latent_dim)\n",
    "    decoder = Decoder(latent_dim)\n",
    "\n",
    "    model = VAE(encoder, decoder).to(device)\n",
    "    if os.path.exists('./best_model.pth'):\n",
    "        model_path = 'best_model.pth'\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    bce_losses = []\n",
    "    kld_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, bce_loss, kld_loss = train(model, dataloader, optimizer, device, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        bce_losses.append(bce_loss)\n",
    "        kld_losses.append(kld_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {train_loss:.4f}, BCE: {bce_loss:.4f}, KLD: {kld_loss:.4f}')\n",
    "        \n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            torch.save(model.state_dict(), f'model_checkpoint_{epoch}.pth')\n",
    "    return train_losses, bce_losses, kld_losses, dataset, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e073478-50e4-4a21-a9ad-a9b2b5464c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6767.2309, BCE: 6637.1291, KLD: 130.1018\n",
      "Epoch 2, Loss: 6767.8547, BCE: 6637.9087, KLD: 129.9460\n",
      "Epoch 3, Loss: 6766.5708, BCE: 6636.2732, KLD: 130.2975\n",
      "Epoch 4, Loss: 6764.4164, BCE: 6633.8240, KLD: 130.5924\n",
      "Epoch 5, Loss: 6764.3766, BCE: 6633.7654, KLD: 130.6112\n",
      "Epoch 6, Loss: 6764.0747, BCE: 6633.3167, KLD: 130.7580\n"
     ]
    }
   ],
   "source": [
    "train_losses, bce_losses, kld_losses, dataset, decoder = main('./dataset', device='mps')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Total Loss')\n",
    "plt.plot(bce_losses, label='BCE Loss')\n",
    "plt.plot(kld_losses, label='KLD Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot_2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ce17a3-d8c2-4d2a-aad8-5d3599928be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7153735d-caef-4ef9-8d30-85ca45741a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 200 \n",
    "# Initialize the Decoder model\n",
    "encoder = Encoder(latent_dim)\n",
    "decoder = Decoder(latent_dim)\n",
    "device = 'mps'\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "model_path = 'best_model.pth'  \n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "new_images = generate_images(model, latent_dim, 16, device)\n",
    "save_image(new_images, 'generated_images.png', nrow=4)\n",
    "\n",
    "start_image = dataset[0].unsqueeze(0).to(device)\n",
    "end_image = dataset[1].unsqueeze(0).to(device)\n",
    "interpolated_images = interpolate_latent(model, start_image, end_image, 10, device)\n",
    "save_image(interpolated_images, 'interpolated_images.png', nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253f505-e351-4165-910c-b6ccacd60c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
